{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb4560a4-0282-4e4d-9a89-c466cb684f99",
   "metadata": {},
   "source": [
    "# Intro to  Neural Networks with PyTorch\n",
    "## Task 1 â€“ Dense Network on Tabular Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b489f09-331e-40ff-a6e8-25adc27eb779",
   "metadata": {},
   "source": [
    "**Objective:** Binary classification to predict whether income exceeds $50K/yr based on census data.\n",
    "\n",
    "**Dataset + Split:** UCI Adult (Census Income) via sklearn.fetch_openml(\"adult\")\n",
    "\n",
    "**Model Idea:** MLP with optional dropout and/or BN\n",
    "\n",
    "**Hyperparameters Chosen:** Depth=2, Width=256, Dropout=0.3, Learning_rate= 1e-3, Weight_decay=1e-4\n",
    "\n",
    "**Results:**\n",
    "\n",
    "**Next Steps:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "9011f0b4-1e0d-40a1-9252-aec542201e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">swept-waterfall-6</strong> at: <a href='https://wandb.ai/sujatha-h-mani/pytorch-bootcamp/runs/ckjcd857' target=\"_blank\">https://wandb.ai/sujatha-h-mani/pytorch-bootcamp/runs/ckjcd857</a><br> View project at: <a href='https://wandb.ai/sujatha-h-mani/pytorch-bootcamp' target=\"_blank\">https://wandb.ai/sujatha-h-mani/pytorch-bootcamp</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251112_153957-ckjcd857/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/sujathahariharan/Desktop/CPS MUL/NEURAL NETWORKS!/Intro to Neural Networks Using PyTorch/wandb/run-20251112_154456-zrdkxb9m</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/sujatha-h-mani/pytorch-bootcamp/runs/zrdkxb9m' target=\"_blank\">silver-resonance-7</a></strong> to <a href='https://wandb.ai/sujatha-h-mani/pytorch-bootcamp' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/sujatha-h-mani/pytorch-bootcamp' target=\"_blank\">https://wandb.ai/sujatha-h-mani/pytorch-bootcamp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/sujatha-h-mani/pytorch-bootcamp/runs/zrdkxb9m' target=\"_blank\">https://wandb.ai/sujatha-h-mani/pytorch-bootcamp/runs/zrdkxb9m</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize wandb run\n",
    "import wandb\n",
    "\n",
    "config = dict(\n",
    "    model=\"mlp\",\n",
    "    depth=2,\n",
    "    width=256,\n",
    "    dropout=0.3,\n",
    "    batch_size=512,\n",
    "    lr=1e-3,\n",
    "    weight_decay=1e-4,\n",
    "    seed=42\n",
    ") # Config for wandb run\n",
    "\n",
    "\n",
    "wandb.login()\n",
    "run = wandb.init(\n",
    "    project=\"pytorch-bootcamp\", \n",
    "    config=config,\n",
    "    tags=[\"task=adult\", \"model=mlp\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb441db-0fa8-49ce-b1c7-bdfbd3805cd7",
   "metadata": {},
   "source": [
    "### Import and Preprocess Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "4674c5e6-dfaa-4a30-b1cf-e7adb074290a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (48842, 15)\n",
      "Target positive rate: 0.239\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25</td>\n",
       "      <td>Private</td>\n",
       "      <td>226802</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>89814</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Farming-fishing</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28</td>\n",
       "      <td>Local-gov</td>\n",
       "      <td>336951</td>\n",
       "      <td>Assoc-acdm</td>\n",
       "      <td>12</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Protective-serv</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  workclass  fnlwgt   education  education-num      marital-status  \\\n",
       "0   25    Private  226802        11th              7       Never-married   \n",
       "1   38    Private   89814     HS-grad              9  Married-civ-spouse   \n",
       "2   28  Local-gov  336951  Assoc-acdm             12  Married-civ-spouse   \n",
       "\n",
       "          occupation relationship   race   sex  capital-gain  capital-loss  \\\n",
       "0  Machine-op-inspct    Own-child  Black  Male             0             0   \n",
       "1    Farming-fishing      Husband  White  Male             0             0   \n",
       "2    Protective-serv      Husband  White  Male             0             0   \n",
       "\n",
       "   hours-per-week native-country target  \n",
       "0              40  United-States  <=50K  \n",
       "1              50  United-States  <=50K  \n",
       "2              40  United-States   >50K  "
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np, pandas as pd\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "# Import dataset\n",
    "adult = fetch_openml('adult', version=2, as_frame=True) # Fetch dataset as a pandas dataframe\n",
    "df = adult.frame.copy()\n",
    "df.rename(columns={\"class\":\"target\"}, inplace=True) # Rename the target column to \"target\"\n",
    "\n",
    "# Remove stray spaces\n",
    "for c in df.columns:\n",
    "    if df[c].dtype == object: # If datatype is text\n",
    "        df[c] = df[c].astype(str).strip() # Remove any blank spaces before or after the text\n",
    "\n",
    "# Separate feature and target variables\n",
    "X = df.drop(columns=[\"target\"]) \n",
    "y = (df[\"target\"] == \">50K\").astype(int) # Convert \">50K\" to 1 and \"<=50K\" to  => \"One-Hot Encoding\"\n",
    "\n",
    "\n",
    "print(\"Shape:\", df.shape)# Check df shape\n",
    "print(\"Target positive rate:\", y.mean().round(3)) # Fraction of people with >50k income\n",
    "df.head(3) # First three rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "b663364b-fb26-4e3f-bc02-4df988edf551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (34189, 14) Val: (7326, 14) Test: (7327, 14)\n",
      "Positive rate (train/val/test): 0.239 0.239 0.239\n"
     ]
    }
   ],
   "source": [
    "# Split into train/test/val\n",
    "from sklearn.model_selection import train_test_split\n",
    "seed = 42\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.30, random_state=seed, stratify=y\n",
    ") # Stratify to maintain similar class balance across splits\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.50, random_state=seed, stratify=y_temp\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Train:\", X_train.shape, \"Val:\", X_val.shape, \"Test:\", X_test.shape)\n",
    "print(\"Positive rate (train/val/test):\", \n",
    "      round(y_train.mean(), 3), round(y_val.mean(), 3), round(y_test.mean(), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "1c004e5b-6645-4f80-ab26-d051e63709a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric columns: ['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week']\n",
      "\n",
      "Categorical columns: ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'native-country']\n"
     ]
    }
   ],
   "source": [
    "# Identify numerical vs categorical terms\n",
    "num_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "cat_cols = [c for c in X_train.columns if c not in num_cols]\n",
    "\n",
    "print(\"Numeric columns:\", num_cols)\n",
    "print(\"\\nCategorical columns:\", cat_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "f113b8c4-58e8-49fd-9a7a-5955810fdfeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Test/Val Preprocessed:\n",
      " (34189, 108) (7326, 108) (7327, 108)\n"
     ]
    }
   ],
   "source": [
    "# Build preprocessing pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", StandardScaler(), num_cols),\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False), cat_cols)\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "# Fit on training data, transform everywhere\n",
    "X_train_np = preprocess.fit_transform(X_train)\n",
    "X_val_np = preprocess.transform(X_val)\n",
    "X_test_np = preprocess.transform(X_test)\n",
    "\n",
    "\n",
    "print(\"Train/Test/Val Preprocessed:\\n\", X_train_np.shape, X_val_np.shape, X_test_np.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "d6fd149d-2b65-453d-bcd9-67843619483a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One batch X: torch.Size([512, 108])  y: torch.Size([512])\n"
     ]
    }
   ],
   "source": [
    "# Convert to PyTorch tensors + DataLoaders\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "def make_loader(X_np, y_s, batch_size=512, shuffle=False):\n",
    "    X_t = torch.from_numpy(X_np.astype(np.float32))\n",
    "    y_t = torch.from_numpy(y_s.values.astype(np.float32))\n",
    "    return DataLoader(TensorDataset(X_t, y_t), batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "train_loader = make_loader(X_train_np, y_train, batch_size=512, shuffle=True)\n",
    "val_loader = make_loader(X_val_np, y_val, batch_size=512, shuffle=False)\n",
    "test_loader = make_loader(X_test_np, y_test, batch_size=512, shuffle=False)\n",
    "\n",
    "batch = next(iter(train_loader))\n",
    "print(\"One batch X:\", batch[0].shape, \" y:\", batch[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7812d76-8701-4df9-80f4-6b41ab7bee0f",
   "metadata": {},
   "source": [
    "### Model Definiton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "a73bf81d-2ca0-4ddc-afab-6077afce77eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the MLP\n",
    "import torch.nn as nn\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_dim, width=256, depth=2, dropout=0.2): # in_dim => Number of input features\n",
    "        super().__init__() # Initialization of parent module (nn.Module)\n",
    "        layers = []\n",
    "        d = in_dim # Current input size to each layer\n",
    "        for _ in range(depth):\n",
    "            layers += [nn.Linear(d, width), nn.ReLU()] # Makes a connection from d inputs -> width neurons; Adds activation function (introduces non-linearity)\n",
    "            if dropout and dropout > 0: # Dropout if present\n",
    "                layers += [nn.Dropout(dropout)]\n",
    "            d = width # Update input size\n",
    "        layers += [nn.Linear(d, 1)] # Final output layer => 1 logit for binary classification\n",
    "        self.net = nn.Sequential(*layers) # Combines all layers into one block and unpacks the list\n",
    "\n",
    "    def forward(self, x): # Moves input through layers of self.net(x)\n",
    "        return self.net(x).squeeze(1) # Removes unnecessary dimension to match what loss function expects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "deab952e-ee66-43c2-9f28-2667680eeb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss, Optimizer, Device\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "input_dim = X_train_np.shape[1] # Number of columns\n",
    "model = MLP(in_dim=input_dim, width=256, depth=2, dropout=0.3).to(device) # Create model using device\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss() # BCE Loss with built-in sigmoid\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4) # Method of optimization of weights during gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "ffa45e66-106f-4934-9556-80e26b4be2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define evaluator for AUC and ACC\n",
    "def evaluate(loader):\n",
    "    model.eval() # Switches model to evaluation mode => Dropout is disabled, BatchNorm uses stored statistics\n",
    "    all_probs, all_targets = [], []\n",
    "    with torch.no_grad(): # Turns off gradient tracking since no backpropagation is used\n",
    "        for xb, yb in loader: # Iterate over mini-batches from DataLoader\n",
    "            xb, yb = xb.to(device), yb.to(device) # Move tensors to same device\n",
    "            logits = model(xb) # Forward pass to get logits\n",
    "            probs = torch.sigmoid(logits) # Convert logits -> probabilities\n",
    "            all_probs.append(probs.cpu()) # Move to CPU for numpy conversion\n",
    "            all_targets.append(yb.cpu())\n",
    "        probs = torch.cat(all_probs).numpy() # Stitch all batches into single NumPy arrays for metric functions\n",
    "        targets = torch.cat(all_targets).numpy()\n",
    "        auc = roc_auc_score(targets, probs)\n",
    "        acc = accuracy_score(targets, (probs >= 0.5).astype(int))\n",
    "        return acc, auc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf28d68-c10b-4cfe-a200-cad3d5467db5",
   "metadata": {},
   "source": [
    "### Training, Validation and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "e5c1afc8-c9bf-488d-94da-0cbc7469de65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | train_loss=0.3926 | val_acc=0.859 | val_auc=0.913\n",
      "Epoch 02 | train_loss=0.3172 | val_acc=0.861 | val_auc=0.915\n",
      "Epoch 03 | train_loss=0.3097 | val_acc=0.862 | val_auc=0.917\n",
      "Epoch 04 | train_loss=0.3067 | val_acc=0.862 | val_auc=0.917\n",
      "Epoch 05 | train_loss=0.3035 | val_acc=0.862 | val_auc=0.917\n",
      "Epoch 06 | train_loss=0.3029 | val_acc=0.861 | val_auc=0.918\n",
      "Epoch 07 | train_loss=0.3020 | val_acc=0.863 | val_auc=0.918\n",
      "Epoch 08 | train_loss=0.2997 | val_acc=0.864 | val_auc=0.917\n",
      "Epoch 09 | train_loss=0.2971 | val_acc=0.863 | val_auc=0.918\n",
      "Epoch 10 | train_loss=0.2962 | val_acc=0.864 | val_auc=0.918\n",
      "\n",
      "Final Evaluation:\n",
      "Best Val AUC=0.918 | Test Acc=0.858 | Test AUC=0.911\n"
     ]
    }
   ],
   "source": [
    "# Train and validate\n",
    "import copy\n",
    "\n",
    "best_state = None\n",
    "best_val_auc = -1.0\n",
    "\n",
    "max_epochs = 10\n",
    "for epoch in range(1, max_epochs + 1):\n",
    "    model.train()\n",
    "    running_loss = 0.0 # Accumulator for total training loss this epoch\n",
    "\n",
    "    # TRAINING\n",
    "    for xb, yb in train_loader: # Loop over training mini-batches and compute in CPU\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad(set_to_none=True) # Reset gradients from previous step\n",
    "        logits = model(xb) # Forward pass\n",
    "        loss = criterion(logits, yb) # Compute BCEWithLogits loss\n",
    "        loss.backward() # Backpropagate to compute gradients of loss w.r.t. every trainable parameter\n",
    "        optimizer.step() # Update weights using AdamW\n",
    "        running_loss += loss.item() * xb.size(0) # Update batch loss\n",
    "        \n",
    "    train_loss = running_loss / len(train_loader.dataset) # Average loss per sample this epoch (total loss sum / no. of samples)\n",
    "\n",
    "    # VALIDATION\n",
    "    val_acc, val_auc = evaluate(val_loader)\n",
    "\n",
    "    # Checkpoint if val AUC improves\n",
    "    if val_auc > best_val_auc:\n",
    "        best_val_auc = val_auc\n",
    "        best_state = copy.deepcopy(model.state_dict()) # Save best weights\n",
    "\n",
    "    # Log progress\n",
    "    print(f\"Epoch {epoch:02d} | \"\n",
    "          f\"train_loss={train_loss:.4f} | \"\n",
    "          f\"val_acc={val_acc:.3f} | \"\n",
    "          f\"val_auc={val_auc:.3f}\")\n",
    "\n",
    "# After training\n",
    "if best_state is not None:\n",
    "    model.load_state_dict(best_state)\n",
    "\n",
    "# TESTING\n",
    "test_acc, test_auc = evaluate(test_loader)\n",
    "print(\"\\nFinal Evaluation:\")\n",
    "print(f\"Best Val AUC={best_val_auc:.3f} | \"\n",
    "      f\"Test Acc={test_acc:.3f} | Test AUC={test_auc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d141e2-ae3a-48d8-996d-ec4993c01768",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
